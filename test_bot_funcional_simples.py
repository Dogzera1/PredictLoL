#!/usr/bin/env python3
"""
Teste Simples de Funcionalidade - Bot LoL V3 Ultra Avan√ßado
Verifica se todos os componentes principais est√£o funcionando

Execu√ß√£o:
    python test_bot_funcional_simples.py
"""

import asyncio
import time
import sys
from pathlib import Path

# Adiciona o diret√≥rio raiz ao path
sys.path.append(str(Path(__file__).parent))

from bot.deployment.production_manager import ProductionManager
from bot.deployment.production_api import ProductionAPI
from bot.monitoring.performance_monitor import PerformanceMonitor
from bot.monitoring.dashboard_generator import DashboardGenerator
from bot.utils.logger_config import get_logger

logger = get_logger(__name__)


class SimpleFunctionalTest:
    """Teste simplificado de funcionalidade"""

    def __init__(self):
        """Inicializa o teste"""
        self.results = []
        
    async def run_tests(self):
        """Executa todos os testes b√°sicos"""
        print("\n" + "="*80)
        print("üß™ BOT LOL V3 ULTRA AVAN√áADO - TESTE DE FUNCIONALIDADE")
        print("üìÖ SEMANA 4: Verifica√ß√£o Simples dos Componentes")
        print("="*80)
        
        tests = [
            ("üì¶ Importa√ß√µes e Depend√™ncias", self.test_imports),
            ("üìä Performance Monitor", self.test_performance_monitor),
            ("üì± Dashboard Generator", self.test_dashboard_generator),
            ("üèóÔ∏è Production Manager - B√°sico", self.test_production_manager_basic),
            ("üåê Production API - B√°sico", self.test_production_api_basic),
            ("üìà Simula√ß√£o de Predi√ß√£o Completa", self.test_prediction_flow),
            ("üíæ Persist√™ncia de Dados", self.test_data_persistence),
        ]
        
        all_passed = True
        
        for test_name, test_func in tests:
            print(f"\nüîç {test_name}...")
            
            try:
                start_time = time.time()
                result = await test_func()
                duration = time.time() - start_time
                
                if result:
                    print(f"‚úÖ {test_name} - PASSOU ({duration:.2f}s)")
                    self.results.append({"test": test_name, "status": "PASS", "duration": duration})
                else:
                    print(f"‚ùå {test_name} - FALHOU ({duration:.2f}s)")
                    self.results.append({"test": test_name, "status": "FAIL", "duration": duration})
                    all_passed = False
                    
            except Exception as e:
                print(f"üí• {test_name} - ERRO: {e}")
                self.results.append({"test": test_name, "status": "ERROR", "error": str(e)})
                all_passed = False
        
        # Relat√≥rio final
        self.print_final_report()
        return all_passed

    async def test_imports(self):
        """Testa importa√ß√µes cr√≠ticas"""
        try:
            # Testa importa√ß√µes b√°sicas
            import aiohttp
            import psutil
            import aiohttp_cors
            import json
            
            # Testa importa√ß√µes do bot
            from bot.monitoring.performance_monitor import PerformanceMonitor, PredictionMetrics
            from bot.monitoring.dashboard_generator import DashboardGenerator
            from bot.deployment.production_manager import ProductionManager
            from bot.deployment.production_api import ProductionAPI
            
            print("   ‚úì Todas as depend√™ncias importadas com sucesso")
            return True
            
        except ImportError as e:
            print(f"   ‚úó Falha na importa√ß√£o: {e}")
            return False

    async def test_performance_monitor(self):
        """Testa PerformanceMonitor b√°sico"""
        try:
            monitor = PerformanceMonitor()
            
            # Cria dados de teste
            fake_prediction = type('obj', (object,), {
                'match_id': 'test_001',
                'predicted_winner': 'Team A',
                'win_probability': 0.75,
                'confidence_level': type('enum', (object,), {'value': 'high'})(),
                'method_used': type('enum', (object,), {'value': 'hybrid'})(),
                'processing_time_ms': 1500.0,
                'ml_prediction': {'features': {'composition_analysis': 0.3}}
            })()
            
            fake_odds = {"outcomes": [{"name": "Team A", "odd": 1.80}]}
            
            # Testa tracking
            pred_id = await monitor.track_prediction(fake_prediction, fake_odds, 10.0)
            if not pred_id:
                print("   ‚úó Falha ao trackear predi√ß√£o")
                return False
            
            # Testa resolu√ß√£o
            success = await monitor.resolve_prediction(pred_id, "Team A")
            if not success:
                print("   ‚úó Falha ao resolver predi√ß√£o")
                return False
            
            # Testa m√©tricas
            metrics = await monitor.get_current_metrics()
            if metrics.total_predictions < 1:
                print("   ‚úó M√©tricas n√£o atualizadas")
                return False
            
            print(f"   ‚úì PerformanceMonitor OK - {metrics.total_predictions} predi√ß√µes, ROI: {metrics.roi_percentage:.1f}%")
            return True
            
        except Exception as e:
            print(f"   ‚úó Erro no PerformanceMonitor: {e}")
            return False

    async def test_dashboard_generator(self):
        """Testa DashboardGenerator"""
        try:
            dashboard_gen = DashboardGenerator()
            
            # Dados de teste
            test_data = {
                "timestamp": "2024-01-01T12:00:00",
                "current_metrics": {"total_predictions": 50, "win_rate": 80.0, "roi": 15.5, "net_profit": 125.0},
                "last_24h": {"predictions": 10, "resolved": 8, "pending": 2, "profit": 25.0},
                "method_performance": {"ml": {"predictions": 20, "win_rate": 85.0}},
                "active_alerts": [],
                "analysis_usage": {"composition_analyses": 30, "avg_processing_time": 1200},
                "trend": {"win_rate_trend": [78, 80], "roi_trend": [14, 15.5]}
            }
            
            # Gera HTML
            html = dashboard_gen.generate_html_dashboard(test_data)
            if len(html) < 5000:
                print("   ‚úó HTML gerado muito pequeno")
                return False
            
            # Verifica elementos essenciais
            if "80.0%" not in html or "125.0" not in html:
                print("   ‚úó Dados n√£o encontrados no HTML")
                return False
            
            print(f"   ‚úì Dashboard HTML gerado ({len(html)} chars)")
            return True
            
        except Exception as e:
            print(f"   ‚úó Erro no DashboardGenerator: {e}")
            return False

    async def test_production_manager_basic(self):
        """Testa ProductionManager b√°sico (sem inicializa√ß√£o completa)"""
        try:
            # Testa apenas cria√ß√£o e configura√ß√£o
            config = {
                "monitoring_interval": 60,
                "health_check_interval": 30,
                "auto_recovery": True
            }
            
            manager = ProductionManager(config=config)
            
            # Verifica se foi criado corretamente
            if not hasattr(manager, 'performance_monitor'):
                print("   ‚úó PerformanceMonitor n√£o foi criado")
                return False
            
            if not hasattr(manager, 'dashboard_generator'):
                print("   ‚úó DashboardGenerator n√£o foi criado")
                return False
            
            # Testa m√©todo de recursos sem inicializar sistema completo
            resources = manager._get_current_resources()
            if resources.cpu_percent < 0 or resources.memory_percent < 0:
                print("   ‚úó Recursos do sistema inv√°lidos")
                return False
            
            print(f"   ‚úì ProductionManager criado - CPU: {resources.cpu_percent:.1f}%, MEM: {resources.memory_percent:.1f}%")
            return True
            
        except Exception as e:
            print(f"   ‚úó Erro no ProductionManager: {e}")
            return False

    async def test_production_api_basic(self):
        """Testa ProductionAPI b√°sico (apenas cria√ß√£o)"""
        try:
            # Cria um manager mock simples
            mock_manager = type('MockManager', (), {
                'performance_monitor': PerformanceMonitor(),
                'dashboard_generator': DashboardGenerator(),
                'get_system_status': lambda: {"status": "healthy"}
            })()
            
            # Testa cria√ß√£o da API
            api = ProductionAPI(
                production_manager=mock_manager,
                host="localhost",
                port=8082
            )
            
            # Verifica se foi criado corretamente
            if not hasattr(api, 'production_manager'):
                print("   ‚úó ProductionManager n√£o foi associado")
                return False
            
            if api.host != "localhost" or api.port != 8082:
                print("   ‚úó Configura√ß√£o de host/port incorreta")
                return False
            
            print(f"   ‚úì ProductionAPI criada - {api.host}:{api.port}")
            return True
            
        except Exception as e:
            print(f"   ‚úó Erro na ProductionAPI: {e}")
            return False

    async def test_prediction_flow(self):
        """Testa fluxo completo de predi√ß√£o"""
        try:
            monitor = PerformanceMonitor()
            dashboard_gen = DashboardGenerator()
            
            # Simula m√∫ltiplas predi√ß√µes
            predictions_data = [
                ("T1", "Gen.G", 0.65, 1.75, "T1"),
                ("DWG", "DRX", 0.72, 1.55, "DWG"),
                ("FNC", "G2", 0.58, 1.95, "G2"),  # Esta perdeu
            ]
            
            total_profit = 0
            
            for i, (team1, team2, prob, odds, winner) in enumerate(predictions_data):
                # Cria predi√ß√£o
                fake_pred = type('obj', (object,), {
                    'match_id': f'flow_test_{i}',
                    'predicted_winner': team1,
                    'win_probability': prob,
                    'confidence_level': type('enum', (object,), {'value': 'high'})(),
                    'method_used': type('enum', (object,), {'value': 'hybrid'})(),
                    'processing_time_ms': 1200.0 + (i * 100),
                    'ml_prediction': {'features': {'composition_analysis': 0.3 + (i * 0.1)}}
                })()
                
                fake_odds = {"outcomes": [{"name": team1, "odd": odds}]}
                
                # Trackea
                pred_id = await monitor.track_prediction(fake_pred, fake_odds, 15.0)
                
                # Resolve
                await monitor.resolve_prediction(pred_id, winner)
                
                # Calcula profit
                is_correct = team1 == winner
                if is_correct:
                    total_profit += (15.0 * odds) - 15.0
                else:
                    total_profit -= 15.0
            
            # Verifica m√©tricas finais
            metrics = await monitor.get_current_metrics()
            if metrics.total_predictions < 3:
                print("   ‚úó N√£o processou todas as predi√ß√µes")
                return False
            
            # Gera dashboard com dados reais
            dashboard_data = monitor.get_live_dashboard_data()
            html = dashboard_gen.generate_html_dashboard(dashboard_data)
            
            if len(html) < 5000:
                print("   ‚úó Dashboard n√£o gerado corretamente")
                return False
            
            print(f"   ‚úì Fluxo completo - {metrics.total_predictions} predi√ß√µes, Win Rate: {metrics.win_rate_percentage:.1f}%, ROI: {metrics.roi_percentage:.1f}%")
            return True
            
        except Exception as e:
            print(f"   ‚úó Erro no fluxo de predi√ß√£o: {e}")
            return False

    async def test_data_persistence(self):
        """Testa persist√™ncia de dados"""
        try:
            # Cria diret√≥rios se n√£o existem
            Path("bot/data/monitoring").mkdir(parents=True, exist_ok=True)
            
            monitor = PerformanceMonitor()
            
            # Adiciona uma predi√ß√£o para ter dados
            fake_pred = type('obj', (object,), {
                'match_id': 'persistence_test',
                'predicted_winner': 'Test Team',
                'win_probability': 0.80,
                'confidence_level': type('enum', (object,), {'value': 'very_high'})(),
                'method_used': type('enum', (object,), {'value': 'hybrid'})(),
                'processing_time_ms': 1000.0,
                'ml_prediction': {'features': {'composition_analysis': 0.4}}
            })()
            
            fake_odds = {"outcomes": [{"name": "Test Team", "odd": 2.0}]}
            pred_id = await monitor.track_prediction(fake_pred, fake_odds, 20.0)
            await monitor.resolve_prediction(pred_id, "Test Team")
            
            # Salva dados
            await monitor._save_performance_data()
            
            # Verifica se arquivos foram criados
            files_to_check = [
                "bot/data/monitoring/predictions.json",
                "bot/data/monitoring/system_metrics.json",
                "bot/data/monitoring/alerts.json"
            ]
            
            for file_path in files_to_check:
                if not Path(file_path).exists():
                    print(f"   ‚úó Arquivo n√£o criado: {file_path}")
                    return False
            
            # Testa carregamento
            new_monitor = PerformanceMonitor()
            await new_monitor._load_historical_data()
            
            if len(new_monitor.predictions) == 0:
                print("   ‚úó Dados n√£o foram carregados")
                return False
            
            print(f"   ‚úì Persist√™ncia OK - {len(new_monitor.predictions)} predi√ß√µes salvas/carregadas")
            return True
            
        except Exception as e:
            print(f"   ‚úó Erro na persist√™ncia: {e}")
            return False

    def print_final_report(self):
        """Imprime relat√≥rio final"""
        print("\n" + "="*80)
        print("üìä RELAT√ìRIO FINAL DE FUNCIONALIDADE")
        print("="*80)
        
        total = len(self.results)
        passed = sum(1 for r in self.results if r["status"] == "PASS")
        failed = sum(1 for r in self.results if r["status"] == "FAIL")
        errors = sum(1 for r in self.results if r["status"] == "ERROR")
        
        print(f"üìà Total de testes: {total}")
        print(f"‚úÖ Passou: {passed}")
        print(f"‚ùå Falhou: {failed}")
        print(f"üí• Erro: {errors}")
        print(f"üìä Taxa de sucesso: {(passed/total)*100:.1f}%")
        
        print(f"\nüìã DETALHES:")
        for result in self.results:
            status_icon = {"PASS": "‚úÖ", "FAIL": "‚ùå", "ERROR": "üí•"}[result["status"]]
            duration = result.get("duration", 0)
            print(f"   {status_icon} {result['test']} ({duration:.2f}s)")
        
        if failed == 0 and errors == 0:
            print(f"\nüéâ BOT 100% FUNCIONAL!")
            print("üöÄ Todos os componentes principais est√£o funcionando perfeitamente!")
            print("‚ú® Sistema pronto para produ√ß√£o!")
        else:
            print(f"\n‚ö†Ô∏è {failed + errors} componente(s) com problemas")
            print("üîß Verificar logs acima para detalhes")
        
        print("="*80)


async def main():
    """Fun√ß√£o principal"""
    try:
        test = SimpleFunctionalTest()
        success = await test.run_tests()
        
        if success:
            print("\nüéâ SISTEMA 100% FUNCIONAL!")
            return 0
        else:
            print("\n‚ö†Ô∏è ALGUNS COMPONENTES PRECISAM DE ATEN√á√ÉO")
            return 1
            
    except KeyboardInterrupt:
        print("\nüõë Teste interrompido")
        return 1
    except Exception as e:
        print(f"\nüí• Erro fatal: {e}")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code) 